[["index.html", "Snakemake &amp; Singulariyt lecture Singularity &amp; Snakemake workflow lecture at ECOFOG. Preface", " Snakemake &amp; Singulariyt lecture Singularity &amp; Snakemake workflow lecture at ECOFOG. Sylvain Schmitt 2022-01-10 Preface This lecture is adapted from the official Snakemake tutorial. Have a nice reading. Sylvain "],["introduction.html", "Introduction", " Introduction This tutorial introduces the text-based workflow system Snakemake. Snakemake follows the GNU Make paradigm: workflows are defined in terms of rules that define how to create output files from input files. Dependencies between the rules are determined automatically, creating a DAG (directed acyclic graph) of jobs that can be automatically parallelized. Snakemake sets itself apart from other text-based workflow systems in the following way. Hooking into the Python interpreter, Snakemake offers a definition language that is an extension of Python with syntax to define rules and workflow specific properties. This allows to combine the flexibility of a plain scripting language with a pythonic workflow definition. The Python language is known to be concise yet readable and can appear almost like pseudo-code. The syntactic extensions provided by Snakemake maintain this property for the definition of the workflow. F urther, Snakemake’s scheduling algorithm can be constrained by priorities, provided cores and customizable resources and it provides a generic support for distributed computing (e.g., cluster or batch systems). Hence, a Snakemake workflow scales without modification from single core workstations and multi-core servers to cluster or batch systems. Finally, Snakemake integrates with the package manager Conda and the container engine Singularity such that defining the software stack becomes part of the workflow itself. The examples presented in this tutorial come from Bioinformatics. However, Snakemake is a general-purpose workflow management system for any discipline. We ensured that no bioinformatics knowledge is needed to understand the tutorial. "],["basics-an-example-workflow.html", "Chapter 1 Basics: An example workflow Background 1.1 Mapping reads 1.2 Generalizing the read mapping rule 1.3 Compressing and sorting read alignments 1.4 Indexing read alignments and visualizing the DAG of jobs 1.5 Calling genomic variants 1.6 Using custom scripts 1.7 Adding a target rule Summary", " Chapter 1 Basics: An example workflow A Snakemake workflow is defined by specifying rules in a Snakefile. Rules decompose the workflow into small steps (for example, the application of a single tool) by specifying how to create sets of output files from sets of input files. Snakemake automatically determines the dependencies between the rules by matching file names. The Snakemake language extends the Python language, adding syntactic structures for rule definition and additional controls. All added syntactic structures begin with a keyword followed by a code block that is either in the same line or indented and consisting of multiple lines. The resulting syntax resembles that of original Python constructs. In the following, we will introduce the Snakemake syntax by creating an example workflow. The workflow comes from the domain of genome analysis. It maps sequencing reads to a reference genome and calls variants on the mapped reads. The tutorial does not require you to know what this is about. Nevertheless, we provide some background in the following paragraph. Background The genome of a living organism encodes its hereditary information. It serves as a blueprint for proteins, which form living cells, carry information and drive chemical reactions. Differences between species, populations or individuals can be reflected by differences in the genome. Certain variants can cause syndromes or predisposition for certain diseases, or cause cancerous growth in the case of tumour cells that have accumulated changes with respect to healthy cells. This makes the genome a major target of biological and medical research. Today, it is often analyzed with DNA sequencing, producing gigabytes of data from a single biological sample (for example a biopsy of some tissue). For technical reasons, DNA sequencing cuts the DNA of a sample into millions of small pieces, called reads. In order to recover the genome of the sample, one has to map these reads against a known reference genome (for example, the human one obtained during the famous human genome project). This task is called read mapping. Often, it is of interest where an individual genome is different from the species-wide consensus represented with the reference genome. Such differences are called variants. They are responsible for harmless individual differences (like eye color), but can also cause diseases like cancer. By investigating the differences between the mapped reads and the reference sequence at a particular genome position, variants can be detected. This is a statistical challenge, because they have to be distinguished from artifacts generated by the sequencing process. 1.1 Mapping reads Our first Snakemake rule maps reads of a given sample to a given reference genome. For this, we will use the tool bwa, specifically the subcommand bwa mem. In the working directory, create a new file called Snakefile using R-studio. In the Snakefile, define the following rule NOTE: A common error is to forget the comma between the input or output items. Since Python concatenates subsequent strings, this can lead to unexpected behavior. A Snakemake rule has a name (here bwa_mem) and a number of directives, here input, output, singularity and shell. The input and output directives are followed by lists of files that are expected to be used or created by the rule. In the simplest case, these are just explicit Python strings. The singularity directive is followed by a Python string containing the address of the container with bwa. The shell directive is followed by a Python string containing the shell command to execute. In the shell command string, we can refer to elements of the rule via braces notation (similar to the Python format function). Here, we refer to the output file by specifying {output} and to the input files by specifying {input}. Since the rule has multiple input files, Snakemake will concatenate them, separated by a whitespace. In other words, Snakemake will replace {input} with data/genome.fa data/samples/A.fastq before executing the command. The shell command invokes bwa mem with reference genome and reads. The output of bwa mem is redirected into the output file defined by the rule with &gt;. When a workflow is executed, Snakemake tries to generate given target files. Target files can be specified via the command line. By executing NOTE: It is best practice to have subsequent steps of a workflow in separate, unique, output folders. This keeps the working directory structured. Further, such unique prefixes allow Snakemake to quickly discard most rules in its search for rules that can provide the requested input. This accelerates the resolution of the rule dependencies in a workflow. in the working directory containing the Snakefile, we tell Snakemake to generate the target file results/mapped_reads/A.sam. Since we used the -n (or --dry-run) flag, Snakemake will only show the execution plan instead of actually performing the steps. The -p flag instructs Snakemake to also print the resulting shell command for illustration. To generate the target files, Snakemake applies the rules given in the Snakefile in a top-down way. The application of a rule to generate a set of output files is called job. For each input file of a job, Snakemake again (i.e. recursively) determines rules that can be applied to generate it. This yields a directed acyclic graph (DAG) of jobs where the edges represent dependencies. So far, we only have a single rule, and the DAG of jobs consists of a single node. Nevertheless, we can execute our workflow with Whenever executing a workflow, you need to specify the number of cores to use. For this tutorial, we will use a single core for now. Later you will see how parallelization works. Note that, after completion of above command, Snakemake will not try to create results/mapped_reads/A.sam again, because it is already present in the file system. Snakemake only re-runs jobs if one of the input files is newer than one of the output files or one of the input files will be updated by another job. 1.2 Generalizing the read mapping rule Obviously, the rule will only work for a single sample with reads in the file data/samples/A.fastq. However, Snakemake allows generalizing rules by using named wildcards. Simply replace the A in the second input file and in the output file with the wildcard {sample}, leading to NOTE: Note that if a rule has multiple output files, Snakemake requires them to all have exactly the same wildcards. Otherwise, it could happen that two jobs running the same rule in parallel want to write to the same file. When Snakemake determines that this rule can be applied to generate a target file by replacing the wildcard {sample} in the output file with an appropriate value, it will propagate that value to all occurrences of {sample} in the input files and thereby determine the necessary input for the resulting job. Note that you can have multiple wildcards in your file paths, however, to avoid conflicts with other jobs of the same rule, all output files of a rule have to contain exactly the same wildcards. When executing Snakemake will determine that the rule bwa_mem can be applied to generate the target file by replacing the wildcard {sample} with the value B. In the output of the dry-run, you will see how the wildcard value is propagated to the input files and all filenames in the shell command. You can also specify multiple targets, for example: Some Bash magic can make this particularly handy. For example, you can alternatively compose our multiple targets in a single pass via Note that this is not a special Snakemake syntax. Bash is just applying its brace expansion to the set {A,B}, creating the given path for each element and separating the resulting paths by a whitespace. In both cases, you will see that Snakemake only proposes to create the output file results/mapped_reads/B.sam. This is because you already executed the workflow before (see the previous step) and no input file is newer than the output file results/mapped_reads/A.sam. You can update the file modification date of the input file data/samples/A.fastq via and see how Snakemake wants to re-run the job to create the file results/mapped_reads/A.sam by executing 1.3 Compressing and sorting read alignments For later steps, we need the read alignments in the BAM files to be sorted. This can be achieved with the samtools sort command. We add the following rule beneath the bwa_map rule: NOTE: In the shell command above we split the string into two lines, which are however automatically concatenated into one by Python. This is a handy pattern to avoid too long shell command lines. When using this, make sure to have a trailing whitespace in each line but the last, in order to avoid arguments to become not properly separated. This rule will take the input file from the mapped_reads directory and store a sorted version in the sorted_reads directory after compression into BAM. Note that Snakemake automatically creates missing directories before jobs are executed. The shell command invokes samtools view with the SAM file which creates a compressed BAM file containing the alignments, and pipes the output into samtools sort. The output of samtools is redirected into the output file defined by the rule with &gt;. For sorting, samtools requires a prefix specified with the flag -T. Here, we need the value of the wildcard sample. Snakemake allows to access wildcards in the shell command via the wildcards object that has an attribute with the value for each wildcard. When issuing you will see how Snakemake wants to run first the rule bwa_mem and then the rule samtools_sort to create the desired target file: as mentioned before, the dependencies are resolved automatically by matching file names. 1.4 Indexing read alignments and visualizing the DAG of jobs Next, we need to use samtools again to index the sorted read alignments so that we can quickly access reads by the genomic location they were mapped to. This can be done with the following rule: NOTE: Snakemake uses the Python format mini language to format shell commands. Sometimes you have to use braces ({}) for something else in a shell command. In that case, you have to escape them by doubling, for example when relying on the bash brace expansion we mentioned above: ls {{A,B}}.txt. Having three steps already, it is a good time to take a closer look at the resulting directed acyclic graph (DAG) of jobs. By executing we create a visualization of the DAG using the dot command provided by Graphviz. For the given target files, Snakemake specifies the DAG in the dot language and pipes it into the dot command, which renders the definition into SVG format. The rendered DAG is piped into the file dag.svg and will look similar to this: The DAG contains a node for each job with the edges connecting them representing the dependencies. The frames of jobs that don’t need to be run (because their output is up-to-date) are dashed. For rules with wildcards, the value of the wildcard for the particular job is displayed in the job node. Exercise Run parts of the workflow using different targets. Recreate the DAG and see how different rules’ frames become dashed because their output is present and up-to-date. 1.5 Calling genomic variants The next step in our workflow will aggregate the mapped reads from all samples and jointly call genomic variants on them. For the variant calling, we will combine the two utilities samtools and bcftools. Snakemake provides a helper function for collecting input files that helps us to describe the aggregation in this step. With we obtain a list of files where the given pattern \"sorted_reads/{sample}.bam\" was formatted with the values in a given list of samples SAMPLES, i.e. The function is particularly useful when the pattern contains multiple wildcards. For example, would create the product of all elements of SAMPLES and the list [0, 1], yielding Here, we use only the simple case of expand. We first let Snakemake know which samples we want to consider. Remember that Snakemake works backwards from requested output, and not from available input. Thus, it does not automatically infer all possible output from, for example, the fastq files in the data folder. Also remember that Snakefiles are in principle Python code enhanced by some declarative statements to define workflows. Hence, we can define the list of samples ad-hoc in plain Python at the top of the Snakefile: NOTE: If you name input or output files like above, their order won’t be preserved when referring to them as {input}. Further, note that named and unnamed (i.e., positional) input and output files can be combined, but the positional ones must come first, equivalent to Python functions with keyword arguments. Later, we will learn about more sophisticated ways like config files. But for now, this is enough so that we can add the following rules to our Snakefile: With multiple input or output files, it is sometimes handy to refer to them separately in the shell command. This can be done by specifying names for input or output files, for example with fa=.... The files can then be referred to in the shell command by name, for example with {input.fa}. Further, you will notice that the input or output file lists can contain arbitrary Python statements, as long as it returns a string, or a list of strings. Here, we invoke our expand function to aggregate over the aligned reads of all samples. Exercise Obtain the updated DAG of jobs for the target file results/calls/all.vcf, it should look like this: 1.6 Using custom scripts Usually, a workflow not only consists of invoking various tools, but also contains custom code to for example calculate summary statistics or create plots. While Snakemake also allows you to directly write Python code inside a rule, it is usually reasonable to move such logic into separate scripts. For this purpose, Snakemake offers the script directive. Add the following rule to your Snakefile: With this rule, we will eventually generate a histogram of the quality scores that have been assigned to the variant calls in the file results/calls/all.vcf. The actual Python code to generate the plot is hidden in the script scripts/plot-quals.R. Script paths are always relative to the referring Snakefile. In the script, all properties of the rule like input, output, wildcards, etc. are available as attributes of a global snakemake object. Create the file scripts/plot-quals.R, with the following content: NOTE: snakemake.input and snakemake.output always contain a list of file names, even if the lists each contain only one file name. Therefore, to refer to a particular file name, you have to index into that list. snakemake.output[0] in Python or snakemake@output[[1]] in R will give you the first element of the output file name list, something that always has to be there. NOTE: It is best practice to use the script directive whenever an inline code block would have more than a few lines of code. Although there are other strategies to invoke separate scripts from your workflow (for example, invoking them via shell commands), the benefit of this is obvious: the script logic is separated from the workflow logic (and can even be shared between workflows), but boilerplate code like the parsing of command line arguments is unnecessary. 1.7 Adding a target rule So far, we always executed the workflow by specifying a target file at the command line. Apart from filenames, Snakemake also accepts rule names as targets if the requested rule does not have wildcards. Hence, it is possible to write target rules collecting particular subsets of the desired results or all results. Moreover, if no target is given at the command line, Snakemake will define the first rule of the Snakefile as the target. Hence, it is best practice to have a rule all at the top of the workflow which has all typically desired target files as input files. Here, this means that we add a rule NOTE: In case you have mutliple reasonable sets of target files, you can add multiple target rules at the top of the Snakefile. While Snakemake will execute the first per default, you can target any of them via the command line (for example, snakemake -n mytarget). the execution plan for creating the file results/plots/quals.png, which contains and summarizes all our results, will be shown. Note that, apart from Snakemake considering the first rule of the workflow as the default target, the order of rules in the Snakefile is arbitrary and does not influence the DAG of jobs. Exercise Create the DAG of jobs for the complete workflow. Execute the complete workflow and have a look at the results/plots/quals.svg. Snakemake provides handy flags for forcing re-execution of parts of the workflow. Have a look at the command line help with snakemake --help and search for the flag --forcerun. Then, use this flag to re-execute the rule samtools_sort and see what happens. With --reason it is possible to display the execution reason for each job. Try this flag together with a dry-run and the --forcerun flag to understand the decisions of Snakemake. Summary In total, the resulting workflow looks like this: "],["advanced-decorating-the-example-workflow.html", "Chapter 2 Advanced: Decorating the example workflow 2.1 Specifying the number of used threads 2.2 Config files 2.3 Input functions &amp; others 2.4 Rule parameters 2.5 Logging 2.6 Temporary and protected files Summary", " Chapter 2 Advanced: Decorating the example workflow Now that the basic concepts of Snakemake have been illustrated, we can introduce some advanced functionality. 2.1 Specifying the number of used threads For some tools, it is advisable to use more than one thread in order to speed up the computation. Snakemake can be made aware of the threads a rule needs with the threads directive. In our example workflow, it makes sense to use multiple threads for the rule bwa_mem: The number of threads can be propagated to the shell command with the familiar braces notation (i.e. {threads}). If no threads directive is given, a rule is assumed to need 1 thread. When a workflow is executed, the number of threads the jobs need is considered by the Snakemake scheduler. In particular, the scheduler ensures that the sum of the threads of all jobs running at the same time does not exceed a given number of available CPU cores. This number is given with the --cores command line argument, which is mandatory for snakemake calls that actually run the workflow. For example NOTE: Apart from the very common thread resource, Snakemake provides a resources directive that can be used to specify arbitrary resources, e.g., memory usage or auxiliary computing devices like GPUs. Similar to threads, these can be considered by the scheduler when an available amount of that resource is given with the command line argument --resources. would execute the workflow with 10 cores. Since the rule bwa_mem needs 3 threads, only one job of the rule can run at a time, and the Snakemake scheduler will try to saturate the remaining cores with other jobs like, e.g., samtools_sort. The threads directive in a rule is interpreted as a maximum: when less cores than threads are provided, the number of threads a rule uses will be reduced to the number of given cores. If --cores is given without a number, all available cores are used. Exercise With the flag --forceall you can enforce a complete re-execution of the workflow. Combine this flag with different values for --cores and examine how the scheduler selects jobs to run in parallel. 2.2 Config files So far, we specified which samples to consider by providing a Python list in the Snakefile. However, often you want your workflow to be customizable, so that it can easily be adapted to new data. For this purpose, Snakemake provides a config file mechanism. Config files can be written in JSON or YAML, and are used with the configfile directive. In our example workflow, we add the line to the top of the Snakefile. Snakemake will load the config file and store its contents into a globally available dictionary named config. In our case, it makes sense to specify the samples in config/config.yaml as Now, we can remove the statement defining SAMPLES from the Snakefile and change the rule samtools_mpileup to 2.3 Input functions &amp; others Since we have stored the path to the FASTQ files in the config file, we can also generalize the rule bwa_mem to use these paths. This case is different to the rule samtools_mpileup we modified above. To understand this, it is important to know that Snakemake workflows are executed in three phases. In the initialization phase, the files defining the workflow are parsed and all rules are instantiated. In the DAG phase, the directed acyclic dependency graph of all jobs is built by filling wildcards and matching input files to output files. In the scheduling phase, the DAG of jobs is executed, with jobs started according to the available resources. The expand functions in the list of input files of the rule samtools_mpileup are executed during the initialization phase. In this phase, we don’t know about jobs, wildcard values and rule dependencies. Hence, we cannot determine the FASTQ paths for rule bwa_mem from the config file in this phase, because we don’t even know which jobs will be generated from that rule. Instead, we need to defer the determination of input files to the DAG phase. This can be achieved by specifying an input function instead of a string as inside of the input directive. For the rule bwa_mem this works as follows: Any normal function would work as well. Input functions take as single argument a wildcards object, that allows to access the wildcards values via attributes (here wildcards.sample). They have to return a string or a list of strings, that are interpreted as paths to input files (here, we return the path that is stored for the sample in the config file). Input functions are evaluated once the wildcard values of a job are determined. Exercise In the data/samples folder, there is an additional sample C.fastq. Add that sample to the config file and see how Snakemake wants to recompute the part of the workflow belonging to the new sample, when invoking with snakemake -n --reason --forcerun samtools_mpileup. Other inputs We saw that inputs can be defined in a configfile and/or using a function. Folder We can also use a folder and list files corresponding to pattern, for instance with FASTQ files here: Later used as : Table We can also use a table and read defined files using the Python library panda, for instance with FASTQ files here: Later used as : Online We can also use a web address, a NCBI request… For instance: 2.4 Rule parameters Sometimes, shell commands are not only composed of input and output files and some static flags. In particular, it can happen that additional parameters need to be set depending on the wildcard values of the job. For this, Snakemake allows to define arbitrary parameters for rules with the params directive. In our workflow, it is reasonable to annotate aligned reads with so-called read groups, that contain metadata like the sample name. We modify the rule bwa_mem accordingly: NOTE: The params directive can also take functions like in Step 3 to defer initialization to the DAG phase. In contrast to input functions, these can optionally take additional arguments input, output, threads, and resources. Similar to input and output files, params can be accessed from the shell command, the Python based run block, or the script directive. Exercise Variant calling can consider a lot of parameters. A particularly important one is the prior mutation rate (1e-3 per default). It is set via the flag -P of the bcftools call command. Consider making this flag configurable via adding a new key to the config file and using the params directive in the rule bcftools_call to propagate it to the shell command. 2.5 Logging When executing a large workflow, it is usually desirable to store the logging output of each job into a separate file, instead of just printing all logging output to the terminal—when multiple jobs are run in parallel, this would result in chaotic output. For this purpose, Snakemake allows to specify log files for rules. Log files are defined via the log directive and handled similarly to output files, but they are not subject of rule matching and are not cleaned up when a job fails. We modify our rule bwa_mem as follows: NOTE: It is best practice to store all log files in a subdirectory logs/, prefixed by the rule or tool name. The shell command is modified to collect STDERR output of bwa and pipe it into the file referred to by {log}. Log files must contain exactly the same wildcards as the output files to avoid file name clashes between different jobs of the same rule. Exercise Add a log directive to the bcftools_call rule as well. Time to re-run the whole workflow (remember the command line flags to force re-execution). See how log files are created for variant calling and read mapping. The ability to track the provenance of each generated result is an important step towards reproducible analyses. Apart from the report functionality discussed after, Snakemake can summarize various provenance information for all output files of the workflow. The flag --summary prints a table associating each output file with the rule used to generate it, the creation date and optionally the version of the tool used for creation is provided. Further, the table informs about updated input files and changes to the source code of the rule after creation of the output file. Invoke Snakemake with --summary to examine the information for our example. 2.6 Temporary and protected files In our workflow, we create two BAM files for each sample, namely the output of the rules bwa_mem and samtools_sort. When not dealing with examples, the underlying data is usually huge. Hence, the resulting BAM files need a lot of disk space and their creation takes some time. To save disk space, you can mark output files as temporary. Snakemake will delete the marked files for you, once all the consuming jobs (that need it as input) have been executed. We use this mechanism for the output file of the rule bwa_mem: This results in the deletion of the BAM file once the corresponding samtools_sort job has been executed. Since the creation of BAM files via read mapping and sorting is computationally expensive, it is reasonable to protect the final BAM file from accidental deletion or modification. We modify the rule samtools_sort to mark its output file as protected: After successful execution of the job, Snakemake will write-protect the output file in the filesystem, so that it can’t be overwritten or deleted by accident. Exercise Re-execute the whole workflow and observe how Snakemake handles the temporary and protected files. Run Snakemake with the target results/mapped_reads/A.bam. Although the file is marked as temporary, you will see that Snakemake does not delete it because it is specified as a target file. Try to re-execute the whole workflow again with the dry-run option. You will see that it fails (as intended) because Snakemake cannot overwrite the protected output files. Summary For this advanced part of the tutorial, we have now created a config/config.yaml configuration file: With this, the final version of our workflow in the Snakefile looks like this: "],["additional-features.html", "Chapter 3 Additional features Automatic reports Benchmarking Modularization Exercise Automatic deployment of software dependencies Tool wrappers Cluster execution Constraining wildcards", " Chapter 3 Additional features Automatic reports Snakemake can automatically create HTML reports with Such a report contains runtime statistics, a visualization of the workflow topology, used software and data provenance information. In addition, you can mark any output file generated in your workflow for inclusion into the report. It will be encoded directly into the report, such that it can be, e.g., emailed as a self-contained document. The reader (e.g., a collaborator of yours) can at any time download the enclosed results from the report for further use, e.g., in a manuscript you write together. In this example, please mark the output file \"results/plots/quals.png\" for inclusion by replacing it with report(\"results/plots/quals.png\", caption=\"report/calling.rst\", category=\"Variants\") and adding a file report/calling.rst, containing some description of the output file. This description will be presented as caption in the resulting report. Benchmarking With the benchmark directive, Snakemake can be instructed to measure the wall clock time of a job. We activate benchmarking for the rule bwa_mem: The benchmark directive takes a string that points to the file where benchmarking results shall be stored. Similar to output files, the path can contain wildcards (it must be the same wildcards as in the output files). When a job derived from the rule is executed, Snakemake will measure the wall clock time and memory usage (in MiB) and store it in the file in tab-delimited format. It is possible to repeat a benchmark multiple times in order to get a sense for the variability of the measurements. This can be done by annotating the benchmark file, e.g., with repeat(\"benchmarks/{sample}.bwa.benchmark.txt\", 3) Snakemake can be told to run the job three times. The repeated measurements occur as subsequent lines in the tab-delimited benchmark file. Modularization In order to re-use building blocks or simply to structure large workflows, it is sometimes reasonable to split a workflow into modules. For this, Snakemake provides the include directive to include another Snakefile into the current one, e.g.: Alternatively, Snakemake allows to define sub-workflows. A sub-workflow refers to a working directory with a complete Snakemake workflow. Output files of that sub-workflow can be used in the current Snakefile. When executing, Snakemake ensures that the output files of the sub-workflow are up-to-date before executing the current workflow. This mechanism is particularly useful when you want to extend a previous analysis without modifying it. For details about sub-workflows, see the documentation. Exercise Put the read mapping related rules into a separate Snakefile and use the include directive to make them available in our example workflow again. Automatic deployment of software dependencies In order to get a fully reproducible data analysis, it is not sufficient to be able to execute each step and document all used parameters. The used software tools and libraries have to be documented as well. Conda can be used to specify an isolated software environment for a whole workflow. With Snakemake, you can go one step further and specify Conda environments per rule. This way, you can even make use of conflicting software versions (e.g. combine Python 2 with Python 3). In our example, instead of using an external environment we can specify environments per rule, e.g.: with envs/samtools.yaml defined as NOTE: The conda directive does not work in combination with run blocks, because they have to share their Python environment with the surrounding snakefile. When Snakemake is executed with It will automatically create required environments and activate them before a job is executed. It is best practice to specify at least the major and minor version of any packages in the environment definition. Specifying environments per rule in this way has two advantages. First, the workflow definition also documents all used software versions. Second, a workflow can be re-executed (without admin rights) on a vanilla system, without installing any prerequisites apart from Snakemake and Miniconda. Tool wrappers In order to simplify the utilization of popular tools, Snakemake provides a repository of so-called wrappers (the Snakemake wrapper repository). A wrapper is a short script that wraps (typically) a command line application and makes it directly addressable from within Snakemake. For this, Snakemake provides the wrapper directive that can be used instead of shell, script, or run. For example, the rule bwa_mem could alternatively look like this: NOTE: Updates to the Snakemake wrapper repository are automatically tested via continuous integration. The wrapper directive expects a (partial) URL that points to a wrapper in the repository. These can be looked up in the corresponding database. The first part of the URL is a Git version tag. Upon invocation, Snakemake will automatically download the requested version of the wrapper. Furthermore, in combination with --use-conda, the required software will be automatically deployed before execution. Cluster execution By default, Snakemake executes jobs on the local machine it is invoked on. Alternatively, it can execute jobs in distributed environments, e.g., compute clusters or batch systems. If the nodes share a common file system, / supports three alternative execution modes. In cluster environments, compute jobs are usually submitted as shell scripts via commands like sbatch. Snakemake provides a generic mode to execute on such clusters. By invoking Snakemake with Each job will be compiled into a shell script that is submitted with the given command (here sbatch). The --jobs flag limits the number of concurrently submitted jobs to 100. This basic mode assumes that the submission command returns immediately after submitting the job. Some clusters allow to run the submission command in synchronous mode, such that it waits until the job has been executed. In such cases, we can invoke e.g. The specified submission command can also be decorated with additional parameters taken from the submitted job. For example, the number of used threads can be accessed in braces similarly to the formatting of shell commands, e.g. To support additional cluster specific parametrization, a Snakefile can be complemented by a Cluster Configuration file, see for instance config/ressources.genologin.yaml that you may declare in the config/config.yaml file as: Finally, the submission fo the workflow can be defined in a separate bash file, see for instance job.sh that you can submit as: Constraining wildcards Snakemake uses regular expressions to match output files to input files and determine dependencies between the jobs. Sometimes it is useful to constrain the values a wildcard can have. This can be achieved by adding a regular expression that describes the set of allowed wildcard values. For example, the wildcard sample in the output file \"results/sorted_reads/{sample}.bam\" can be constrained to only allow alphanumeric sample names as \"results/sorted_reads/{sample,[A-Za-z0-9]+}.bam\". Constraints may be defined per rule or globally using the wildcard_constraints keyword. This mechanism helps to solve two kinds of ambiguity. It can help to avoid ambiguous rules, i.e. two or more rules that can be applied to generate the same output file. Other ways of handling ambiguous rules are described in the Section Handling Ambiguous Rules. It can help to guide the regular expression based matching so that wildcards are assigned to the right parts of a file name. Consider the output file {sample}.{group}.txt and assume that the target file is A.1.normal.txt. It is not clear whether dataset=\"A.1\" and group=\"normal\" or dataset=\"A\" and group=\"1.normal\" is the right assignment. Here, constraining the dataset wildcard by {sample,[A-Z]+}.{group} solves the problem. When dealing with ambiguous rules, it is best practice to first try to solve the ambiguity by using a proper file structure, for example, by separating the output files of different steps in different directories. "],["singularity.html", "Chapter 4 Singularity 4.1 Part Summary", " Chapter 4 Singularity 4.1 Part Summary "],["conclusion.html", "Chapter 5 Conclusion", " Chapter 5 Conclusion "]]
